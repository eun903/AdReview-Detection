import mysql.connector
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import ttest_ind
import random
import json
import os

# ============================
# DB ì„¤ì •
# ============================
DB_CONFIG = {
    "host": "localhost",
    "user": "root",
    "password": "onlyroot",
    "database": "comparation",
    "auth_plugin": 'mysql_native_password',
    "charset": 'utf8mb4'
}

# ============================
# í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¡œë“œ
# ============================
with open("keyword_weights.json", "r", encoding="utf-8") as f:
    weights_data = json.load(f)

AD_WEIGHTS = weights_data["AD_WEIGHTS"]
NON_AD_WEIGHTS = weights_data["NON_AD_WEIGHTS"]

# ============================
# í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš© ì„ë² ë”© í•¨ìˆ˜
# ============================
def compute_weighted_embeddings(reviews, model, weight_dict):
    embeddings = []
    for review in reviews:
        emb = model.encode(review, convert_to_numpy=True)
        weight = 1.0
        for kw, w in weight_dict.items():
            if kw in review:
                weight += w
        embeddings.append(emb * weight)
    return np.array(embeddings)

# ============================
# íŒŒì¸íŠœë‹
# ============================
def finetune_model(db_config, output_model_path="./finetuned_minilm_model", epochs=3):
    db = mysql.connector.connect(**db_config)
    cursor = db.cursor(dictionary=True)
    cursor.execute("SELECT id, cleaned_review, label FROM reviews WHERE cleaned_review IS NOT NULL")
    rows = cursor.fetchall()
    cursor.close()
    db.close()

    ads = [r for r in rows if r["label"] == 1]
    non_ads = [r for r in rows if r["label"] == 0]

    train_examples = []

    # ê´‘ê³ -ê´‘ê³  ìœ ì‚¬ë„=1.0
    for _ in range(len(ads)//2):
        a, b = random.sample(ads, 2)
        train_examples.append(InputExample(texts=[a["cleaned_review"], b["cleaned_review"]], label=1.0))

    # ë¹„ê´‘ê³ -ë¹„ê´‘ê³  ìœ ì‚¬ë„=1.0
    for _ in range(len(non_ads)//2):
        a, b = random.sample(non_ads, 2)
        train_examples.append(InputExample(texts=[a["cleaned_review"], b["cleaned_review"]], label=1.0))

    # ê´‘ê³ -ë¹„ê´‘ê³  ìœ ì‚¬ë„=0.0
    for _ in range(min(len(ads), len(non_ads))):
        a = random.choice(ads)
        b = random.choice(non_ads)
        train_examples.append(InputExample(texts=[a["cleaned_review"], b["cleaned_review"]], label=0.0))

    print(f"ì´ í•™ìŠµ ìƒ˜í”Œ: {len(train_examples)} ê°œ")

    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
    model = SentenceTransformer("all-MiniLM-L6-v2")
    train_loss = losses.CosineSimilarityLoss(model)

    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=int(len(train_dataloader) * 0.1),
        output_path=output_model_path
    )

    print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {output_model_path}")
    return model

# ============================
# ë¦¬ë·° ë²¡í„° DB ì—…ë°ì´íŠ¸ (ì´ë¯¸ ìˆëŠ” ë²¡í„° ê±´ë„ˆë›°ê¸°)
# ============================
def update_review_vectors(db_config, model):
    db = mysql.connector.connect(**db_config)
    cursor = db.cursor(dictionary=True)
    
    cursor.execute("SELECT id, cleaned_review FROM reviews WHERE review_vector IS NULL OR review_vector = ''")
    rows = cursor.fetchall()
    print(f"ì´ {len(rows)}ê°œ ë¦¬ë·° ë²¡í„° ìƒì„± ì˜ˆì •")

    count = 0
    for row in rows:
        review_id = row["id"]
        text = (row["cleaned_review"] or "").strip()
        if not text:
            continue
        vector = model.encode(text).tolist()
        vector_json = json.dumps(vector, ensure_ascii=False)
        cursor.execute("UPDATE reviews SET review_vector = %s WHERE id = %s", (vector_json, review_id))
        count += 1
        if count % 100 == 0:
            db.commit()
            print(f"  - {count}ê°œ ì»¤ë°‹ ì™„ë£Œ")
    db.commit()
    cursor.close()
    db.close()
    print(f"âœ… ë²¡í„° ìƒì„± ì™„ë£Œ: ì´ {count}ê°œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

# ============================
# í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš© í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
# ============================
def run_weighted_similarity_analysis_from_db(db_config, model):
    db = None
    try:
        db = mysql.connector.connect(**db_config)
        cursor = db.cursor(dictionary=True)
        cursor.execute("SELECT cleaned_review, label FROM reviews WHERE cleaned_review IS NOT NULL")
        rows = cursor.fetchall()
        cursor.close()

        data = pd.DataFrame(rows)
        data = data.dropna(subset=['cleaned_review', 'label'])
        data['label_text'] = data['label'].astype(str).replace({'1': 'ê´‘ê³ ', '0': 'ë¹„ê´‘ê³ '})

        ads = data[data["label_text"] == "ê´‘ê³ "]["cleaned_review"].astype(str).tolist()
        non_ads = data[data["label_text"] == "ë¹„ê´‘ê³ "]["cleaned_review"].astype(str).tolist()

        print(f"âœ… DBì—ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")
        print(f"ê´‘ê³  ë¦¬ë·° ìˆ˜: {len(ads)}ê°œ, ë¹„ê´‘ê³  ë¦¬ë·° ìˆ˜: {len(non_ads)}ê°œ")
        if len(ads) < 2 or len(non_ads) < 2:
            print("ë°ì´í„° ë¶€ì¡±")
            return

    except Exception as e:
        print(f"âŒ DB ì˜¤ë¥˜: {e}")
        return
    finally:
        if db and db.is_connected():
            db.close()

    try:
        print("\nâ³ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© + í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©")
        emb_ads = compute_weighted_embeddings(ads, model, AD_WEIGHTS)
        emb_non_ads = compute_weighted_embeddings(non_ads, model, NON_AD_WEIGHTS)

        mean_ads = np.mean(emb_ads, axis=0)
        mean_non_ads = np.mean(emb_non_ads, axis=0)

        sim_ads = cosine_similarity(emb_ads, [mean_ads])[:, 0]
        sim_non_ads = cosine_similarity(emb_non_ads, [mean_non_ads])[:, 0]

        mean_ads_score = np.mean(sim_ads)
        mean_non_ads_score = np.mean(sim_non_ads)
        std_ads = np.std(sim_ads)
        std_non_ads = np.std(sim_non_ads)

        print("\nğŸ“ˆ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ + íŒŒì¸íŠœë‹ í›„ í‰ê·  ìœ ì‚¬ë„")
        print(f"ê´‘ê³  ë¦¬ë·° í‰ê·  ìœ ì‚¬ë„: {mean_ads_score:.3f} Â± {std_ads:.3f}")
        print(f"ë¹„ê´‘ê³  ë¦¬ë·° í‰ê·  ìœ ì‚¬ë„: {mean_non_ads_score:.3f} Â± {std_non_ads:.3f}")
        print(f"ì°¨ì´ (ê´‘ê³  - ë¹„ê´‘ê³ ): {mean_ads_score - mean_non_ads_score:.3f}")

        t_stat, p_value = ttest_ind(sim_ads, sim_non_ads, equal_var=False)
        print("\nğŸ§® Welch's t-test")
        print(f"t í†µê³„ëŸ‰: {t_stat:.3f}, p-value: {p_value:.6f}")

        n1, n2 = len(sim_ads), len(sim_non_ads)
        s_pooled = np.sqrt(((n1-1)*std_ads**2 + (n2-1)*std_non_ads**2) / (n1 + n2 - 2))
        cohen_d = (mean_ads_score - mean_non_ads_score) / s_pooled
        print(f"Cohen's d: {cohen_d:.3f}")

    except Exception as e:
        print(f"âŒ ìœ ì‚¬ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")

# ============================
# ì‹¤í–‰
# ============================
if __name__ == "__main__":
    # 1ï¸âƒ£ íŒŒì¸íŠœë‹ ëª¨ë¸ í•™ìŠµ (ì´ë¯¸ í•™ìŠµí•œ ëª¨ë¸ ìˆìœ¼ë©´ skip ê°€ëŠ¥)
    if os.path.exists("./finetuned_minilm_model"):
        print("âœ… ê¸°ì¡´ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ")
        model = SentenceTransformer("./finetuned_minilm_model")
    else:
        model = finetune_model(DB_CONFIG, output_model_path="./finetuned_minilm_model", epochs=3)

    # 2ï¸âƒ£ ë²¡í„° ì—…ë°ì´íŠ¸
    update_review_vectors(DB_CONFIG, model)

    # 3ï¸âƒ£ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš© í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
    run_weighted_similarity_analysis_from_db(DB_CONFIG, model)
